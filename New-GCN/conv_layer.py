import tensorflow as tf

_LAYER_IDS = {}


def get_layer_id(layer=''):
    """
    Assigns and retrieves unique layer IDs based on the provided layer name.

    :param layer: A string representing the name of the layer for which a unique ID is assigned.
    :return: A unique layer ID associated with the provided layer name.

    This function helps manage unique layer IDs to ensure each layer in a neural network has a distinct identifier.
    If the layer_name is encountered for the first time, a new ID is created and returned (starting from 1).
    If the layer_name already has an associated ID, the function increments the existing ID and returns the updated value.

    :Example:

    >>> get_layer_id('dense_layer')
    1
    >>> get_layer_id('conv_layer')
    1
    >>> get_layer_id('dense_layer')
    2
    """

    if layer not in _LAYER_IDS:
        _LAYER_IDS[layer] = 1
        return 1
    else:
        _LAYER_IDS[layer] += 1
        return _LAYER_IDS[layer]


def sparse_dropout(input_tensor, keep_prob, noise_shape):
    """
    Applies dropout to sparse tensors.

    :param tf.SparseTensor input_tensor:
        The input sparse tensor.
    :param float keep_prob:
        The probability that each element is kept.
    :param noise_shape: 1D tensor
        A 1D tensor of type int representing the shape for randomly generated keep/drop flags.

    :return: tf.SparseTensor
        A sparse tensor with dropout applied.

    This function applies dropout to a sparse tensor `x`. It randomly drops elements based on the provided
    `keep_prob` probability and `noise_shape`.

    The `random_tensor` is generated by adding `keep_prob` to uniformly distributed random values in the range [0, 1)
    with the shape defined by `noise_shape`. Elements in the resulting `random_tensor` that are greater than or equal
    to 1.0 are kept, and others are dropped.

    The `dropout_mask` is created by casting the floor of `random_tensor` to boolean values.

    The `pre_out` sparse tensor is obtained by retaining elements in the input sparse tensor `x` based on the
    `dropout_mask`.

    The final output is obtained by scaling `pre_out` with `(1. / keep_prob)`.

    Example:
    >>> input_tensor = tf.SparseTensor(...)
    >>> keep_prob = 0.8
    >>> noise_shape = tf.constant([10])
    >>> result = sparse_dropout(input_tensor, keep_prob, noise_shape)
    """
    rand_tensor = keep_prob + tf.random.uniform(shape=noise_shape)
    dropout_mask = tf.cast(tf.floor(rand_tensor), dtype=tf.bool)
    pre_out = tf.sparse.retain(input_tensor, dropout_mask)
    return pre_out * (1. / keep_prob)


class Layer():
    """Base layer class. Defines basic API for all layer objects.
    Implementation inspired by keras (http://keras.io).

    # Properties
        name: String, defines the variable scope of the layer.
        logging: Boolean, switches Tensorflow histogram logging on/off

    # Methods
        _call(inputs): Defines computation graph of layer
            (i.e. takes input, returns output)
        __call__(inputs): Wrapper for _call()
        _log_vars(): Log all variables
    """

    def __init__(self, **kwargs):
        allowed_kwargs = {'name', 'logging'}
        for kwarg in kwargs.keys():
            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg
        name = kwargs.get('name')
        if not name:
            layer = self.__class__.__name__.lower()
            name = layer + '_' + str(get_layer_uid(layer))
        self.name = name
        self.vars = {}
        logging = kwargs.get('logging', False)
        self.logging = logging
        self.sparse_inputs = False

    def _call(self, inputs):
        return inputs

    def __call__(self, inputs):
        with tf.name_scope(self.name):
            if self.logging and not self.sparse_inputs:
                tf.summary.histogram(self.name + '/inputs', inputs)
            outputs = self._call(inputs)
            if self.logging:
                tf.summary.histogram(self.name + '/outputs', outputs)
            return outputs

    def _log_vars(self):
        for var in self.vars:
            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])


class GraphConvolution():
    """Graph convolution layer."""

    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,
                 sparse_inputs=False, act=tf.nn.relu, bias=False,
                 featureless=False, **kwargs):

        allowed_kwargs = {'name', 'logging'}
        for kwarg in kwargs.keys():
            assert kwarg in allowed_kwargs, 'Invalid argument: ' + kwarg
        name = kwargs.get('name', 'graphconvolution_' + str(get_layer_uid('graphconvolution')))
        logging = kwargs.get('logging', False)

        self.name = name
        self.vars = {}
        self.logging = logging
        self.dropout = placeholders['dropout'] if dropout else 0.
        self.act = act
        self.support = placeholders['support']

        self.sparse_inputs = sparse_inputs
        self.featureless = featureless
        self.bias = bias
        self.num_nonzero_features = placeholders['num_features_nonzero']

        with tf.compat.v1.variable_scope(self.name + '_vars'):
            for i in range(len(self.support)):
                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],
                                                        name='weights_' + str(i))
            if self.bias:
                self.vars['bias'] = zeros([output_dim], name='bias')

        if self.logging:
            self._log_vars()

    def _call(self, inputs):
        x = inputs

        # dropout
        if self.sparse_inputs:
            x = sparse_dropout(x, 1 - self.dropout, self.num_nonzero_features)
        else:
            x = tf.compat.v1.nn.dropout(x, rate=self.dropout)

        # convolve
        supports = list()
        for i in range(len(self.support)):
            if not self.featureless:
                pre_sup = dot(x, self.vars['weights_' + str(i)],
                              sparse=self.sparse_inputs)
            else:
                pre_sup = self.vars['weights_' + str(i)]
            support = dot(self.support[i], pre_sup, sparse=True)
            supports.append(support)
        output = tf.add_n(supports)

        # bias
        if self.bias:
            output += self.vars['bias']

        return self.act(output)

    def _log_vars(self):
        for var in self.vars:
            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])
